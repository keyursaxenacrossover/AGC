 EFS reads out and processes or data that is written as a result of processing).</em></p>
<p><em>Basically, you cannot say across the board that there is a direct connection between the number of connections or traffic and CPU utilization on the DB server or FPMPOOL side. A connection can contain a very simple or a very complex database query, which then requires very little or a lot of CPU, or just a single query or a million queries where the bulk of the CPU is used. Here there were many connections, but they required few resources.</em></p>
<p><em>When it comes to traffic, an export or import can of course differ greatly compared to the few bytes that are transferred on many survey pages, especially when you have already left the home page and the majority is already in the cache.</em></p>
<p><em>Tracing when which action caused which peak can only be found out by very time-consuming analysis of the log files. Since there were never any performance bottlenecks of any kind, I would refrain from carrying out such an analysis. Everything is within a normal range. Broken down from the participant's point of view, the only problem is if the pods permanently reach the maximum of 40 or the CPU is permanently at 100%.</em></p>
<h1 id="h_01J7XNEHYQNB7YT0FYWPTBKYGS">Troubleshooting</h1>
<p>It could also happen that things do not go as planned, and the customer reports issues with their performance OR SaaS receives an alert related to the good standing of the instances.</p>
<p>With the current accesses, you are only able to access Grafana and take a peek at the dashboards, afterwich you need to <a href="https://trilogy-eng.atlassian.net/jira/software/c/projects/DXI/issues" target="_blank" rel="noopener noreferrer"><strong>raise a SaaS incident</strong></a> for investigating the issue.</p>></p>
<p><em>When it comes to traffic, an export or import can of course differ greatly compared to the few bytes that are transferred on many survey pages, especially when you have already left the home page and the majority is already in the cache.</em></p>
<p><em>Tracing when which action caused which peak can only be found out by very time-consuming analysis of the log files. Since there were never any performance bottlenecks of any kind, I would refrain from carrying out such an analysis. Everything is within a normal range. Broken down from the participant's point of view, the only problem is if the pods permanently reach the maximum of 40 or the CPU is permanently at 100%.</em></p>
<h1 id="h_01J7XNEHYQNB7YT0FYWPTBKYGS">Troubleshooting</h1>
<p>It could also happen that things do not go as planned, and the customer reports issues with their performance OR SaaS receives an alert related to the good standing of the instances.</p>
<p>With the current accesses, you are only able to access Grafana and take a peek at the dashboards, afterwich you need to <a href="https://trilogy-eng.atlassian.net/jira/software/c/projects/DXI/issues" target="_blank" rel="noopener noreferrer"><strong>raise a SaaS incident</strong></a> for investigating the issue.</p>